{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"./Desktop/Dataset/Training/\"\n",
    "filepaths_train = []\n",
    "labels_train = []\n",
    "\n",
    "folders_train = os.listdir(train_data)\n",
    "for folder in folders_train:\n",
    "    folderpath = os.path.join(train_data, folder)\n",
    "    filelist = os.listdir(folderpath)\n",
    "    for file in filelist:\n",
    "        fpath = os.path.join(folderpath, file)\n",
    "\n",
    "        filepaths_train.append(fpath)\n",
    "        labels_train.append(folder)\n",
    "\n",
    "FSeries_train = pd.Series(filepaths_train, name=\"filepaths\")\n",
    "LSeries_train = pd.Series(labels_train, name=\"labels\")\n",
    "\n",
    "train_df = pd.concat([FSeries_train, LSeries_train], axis=1)\n",
    "\n",
    "\n",
    "test_data = \"./Desktop/Dataset/Testing/\"\n",
    "filepaths_test = []\n",
    "labels_test = []\n",
    "\n",
    "folders_test = os.listdir(test_data)\n",
    "for folder in folders_test:\n",
    "    folderpath = os.path.join(test_data, folder)\n",
    "    filelist = os.listdir(folderpath)\n",
    "    for file in filelist:\n",
    "        fpath = os.path.join(folderpath, file)\n",
    "\n",
    "        filepaths_test.append(fpath)\n",
    "        labels_test.append(folder)\n",
    "\n",
    "\n",
    "FSeries_test = pd.Series(filepaths_test, name=\"filepaths\")\n",
    "LSeries_test = pd.Series(labels_test, name=\"labels\")\n",
    "\n",
    "tst_df = pd.concat([FSeries_test, LSeries_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balance = train_df.labels.value_counts()\n",
    "sns.set_palette(\"flare\")\n",
    "\n",
    "\n",
    "def custom_autopct(pct):\n",
    "    total = sum(data_balance)\n",
    "    val = int(round(pct * total / 100.0))\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, val)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(data_balance, labels=data_balance.index, autopct=custom_autopct)\n",
    "\n",
    "plt.title(\"Training Data Balance\", weight=\"bold\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df, test_df = train_test_split(tst_df,  train_size= 0.5, shuffle= True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_balance_train = len(train_df.value_counts())\n",
    "data_balance_valid = len(valid_df.value_counts())\n",
    "data_balance_test = len(test_df.value_counts())\n",
    "\n",
    "data_balance = [data_balance_train, data_balance_valid, data_balance_test]\n",
    "\n",
    "labels = [\"Train Data\", \"Valid Data\", \"Test Data\"]\n",
    "\n",
    "plt.title(\"Train - Val - Test Balance\", weight=\"bold\")\n",
    "plt.pie(data_balance, labels=labels, autopct=custom_autopct)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a439703",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "img_size = (224, 224)\n",
    "\n",
    "tr_gen = ImageDataGenerator()\n",
    "tst_gen = ImageDataGenerator()\n",
    "\n",
    "train_gen = tr_gen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col=\"filepaths\",\n",
    "    y_col=\"labels\",\n",
    "    target_size=img_size,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "valid_gen = tst_gen.flow_from_dataframe(\n",
    "    valid_df,\n",
    "    x_col=\"filepaths\",\n",
    "    y_col=\"labels\",\n",
    "    target_size=img_size,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "test_gen = tst_gen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col=\"filepaths\",\n",
    "    y_col=\"labels\",\n",
    "    target_size=img_size,\n",
    "    class_mode=\"categorical\",\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", input_shape= (224,224,3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),  \n",
    "\n",
    "        Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),  \n",
    "\n",
    "        Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),  \n",
    "        \n",
    "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),  \n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(256,activation = \"relu\"),\n",
    "        Dense(64,activation = \"relu\"),\n",
    "        Dense(4, activation = \"softmax\")\n",
    "    ])\n",
    "\n",
    "model.compile(Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_gen, epochs=epochs, verbose=1, validation_data=valid_gen, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fb0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet=Sequential()\n",
    "alexnet.add(Conv2D(96,kernel_size=(11,11),strides=(4,4),activation='relu',input_shape=(224,224,3)))\n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "alexnet.add(ZeroPadding2D((2,2)))\n",
    "alexnet.add(Conv2D(256,kernel_size=(5,5),activation='relu',strides=(1,1)))\n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "alexnet.add(ZeroPadding2D((1,1)))\n",
    "alexnet.add(Conv2D(384,kernel_size=(3,3),activation='relu',strides=(1,1)))\n",
    "alexnet.add(ZeroPadding2D((1,1)))\n",
    "alexnet.add(Conv2D(384,kernel_size=(3,3),activation='relu',strides=(1,1)))\n",
    "alexnet.add(ZeroPadding2D((1,1)))\n",
    "alexnet.add(Conv2D(256,kernel_size=(3,3),activation='relu',strides=(1,1)))\n",
    "alexnet.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "alexnet.add(Flatten())\n",
    "alexnet.add(Dense(4096,activation='relu'))\n",
    "alexnet.add(Dense(4096,activation='relu'))\n",
    "alexnet.add(Dense(4,activation='softmax'))\n",
    "\n",
    "alexnet.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "history=alexnet.fit(train_gen, epochs=epochs, verbose=1, validation_data=valid_gen, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = Sequential()\n",
    "vgg16.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(Conv2D(64, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg16.add(Conv2D(128, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(Conv2D(128, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg16.add(Conv2D(256, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(Conv2D(256, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(Conv2D(256, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg16.add(Conv2D(512, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(Conv2D(512, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(Conv2D(512, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg16.add(Conv2D(512, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(Conv2D(512, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(Conv2D(512, kernel_size=(3,3),padding='same',activation='relu'))\n",
    "vgg16.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "vgg16.add(Flatten())\n",
    "vgg16.add(Dense(4096,activation='relu'))\n",
    "vgg16.add(Dense(4096,activation='relu'))\n",
    "vgg16.add(Dense(4,activation='softmax'))\n",
    "\n",
    "vgg16.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "history=vgg16.fit(train_gen, epochs=epochs, verbose=1, validation_data=valid_gen, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e97b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define needed variables\n",
    "tr_acc = history.history['accuracy']\n",
    "tr_loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "index_loss = np.argmin(val_loss)\n",
    "val_lowest = val_loss[index_loss]\n",
    "index_acc = np.argmax(val_acc)\n",
    "acc_highest = val_acc[index_acc]\n",
    "\n",
    "Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize= (20, 8))\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40513be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(train_gen, verbose=1)\n",
    "valid_score = model.evaluate(valid_gen, verbose=1)\n",
    "test_score = model.evaluate(test_gen, verbose=1)\n",
    "\n",
    "print(f\"Train loss: {train_score[0]}\")\n",
    "print(f\"Train Accuracy: {train_score[1]}\")\n",
    "print(\"-\" * 20)\n",
    "print( )\n",
    "print(f\"Validation Loss: {valid_score[0]}\")\n",
    "print(f\"Validation Accuracy: {valid_score[1]}\")\n",
    "print(\"-\" * 20)\n",
    "print( )\n",
    "print(f\"Test Loss: {test_score[0]}\")\n",
    "print(f\"Test Accuracy: {test_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe05e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = alexnet.evaluate(train_gen, verbose=1)\n",
    "valid_score = alexnet.evaluate(valid_gen, verbose=1)\n",
    "test_score = alexnet.evaluate(test_gen, verbose=1)\n",
    "\n",
    "print(f\"Train loss: {train_score[0]}\")\n",
    "print(f\"Train Accuracy: {train_score[1]}\")\n",
    "print(\"-\" * 20)\n",
    "print( )\n",
    "print(f\"Validation Loss: {valid_score[0]}\")\n",
    "print(f\"Validation Accuracy: {valid_score[1]}\")\n",
    "print(\"-\" * 20)\n",
    "print( )\n",
    "print(f\"Test Loss: {test_score[0]}\")\n",
    "print(f\"Test Accuracy: {test_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac26ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = vgg16.evaluate(train_gen, verbose=1)\n",
    "valid_score = vgg16.evaluate(valid_gen, verbose=1)\n",
    "test_score = vgg16.evaluate(test_gen, verbose=1)\n",
    "\n",
    "print(f\"Train loss: {train_score[0]}\")\n",
    "print(f\"Train Accuracy: {train_score[1]}\")\n",
    "print(\"-\" * 20)\n",
    "print( )\n",
    "print(f\"Validation Loss: {valid_score[0]}\")\n",
    "print(f\"Validation Accuracy: {valid_score[1]}\")\n",
    "print(\"-\" * 20)\n",
    "print( )\n",
    "print(f\"Test Loss: {test_score[0]}\")\n",
    "print(f\"Test Accuracy: {test_score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19760a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_gen)  \n",
    "y_pred = np.argmax(preds, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd975948",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = alexnet.predict(test_gen)  \n",
    "y_pred = np.argmax(preds, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vgg16.predict(test_gen)  \n",
    "y_pred = np.argmax(preds, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = test_gen.class_indices\n",
    "classes = list(g_dict.keys())\n",
    "\n",
    "cm = confusion_matrix(test_gen.classes, y_pred)\n",
    "\n",
    "plt.figure(figsize= (10, 10))\n",
    "plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation= 45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(test_gen.classes, y_pred, target_names=classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
